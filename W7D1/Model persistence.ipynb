{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we need to serialize or save models\n",
    "* Serialization is the process of converting an object into a stream of bytes to store the object or transmit it to memory, a database, or a file. Its main purpose is to save the state of an object in order to be able to recreate it when needed.\n",
    "\n",
    "* We need serialization because models take time to train and we may train many models over different seed values and different data and we can save these to reporduce the same results\n",
    "\n",
    "* We need saved states of models trained to deploy models which can be used by a business or end user/customer to perform predictions\n",
    "\n",
    "* Think of a weather prediction model you created. Now that you have trained and created a model that predicts weather accurately it should have the weights and hyperparameters of the model saved in a format that the model when loaded is ready to predict right away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./picklerick.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pickling is the process whereby a Python object hierarchy is converted into a byte stream (usually not human readable) \n",
    "\n",
    "* Unpickling is the reverse operation, whereby a byte stream is converted back into a working Python object hierarchy.\n",
    "\n",
    "* Pickle is operationally simplest way to store the object. \n",
    "\n",
    "* The Python Pickle module is an object-oriented way to store objects directly in a special storage format.\n",
    "\n",
    "[source: 'https://www.tutorialspoint.com/object_oriented_python/object_oriented_python_serialization.htm' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can it do?\n",
    "* Pickle can store and reproduce dictionaries and lists very easily.\n",
    "* Stores object attributes and restores them back to the same State.\n",
    "\n",
    "### What pickle can’t do?\n",
    "* It does not save an objects code. Only it’s attributes values.\n",
    "* It cannot store file handles or connection sockets.\n",
    "* In short we can say, pickling is a way to store and retrieve data variables into and out from files where variables can be lists, classes, etc.\n",
    "\n",
    "### To Pickle something you must −\n",
    "\n",
    "<code>import pickle\n",
    "Write a variable to file, something like\n",
    "pickle.dump(mystring, outfile, protocol)</code>\n",
    "\n",
    "#### Methods\n",
    "The pickle interface provides four different methods.\n",
    "\n",
    "dump() − The dump() method serializes to an open file (file-like object).\n",
    "\n",
    "dumps() − Serializes to a string\n",
    "\n",
    "load() − Deserializes from an open-like object.\n",
    "\n",
    "loads() − Deserializes from a string.\n",
    "\n",
    "[source: 'https://www.tutorialspoint.com/object_oriented_python/object_oriented_python_serialization.htm' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Here's an example dict\n",
    "grades = { 'Alice': 89, 'Bob': 72, 'Charles': 87 }\n",
    "\n",
    "#Use dumps to convert the object to a serialized string\n",
    "serial_grades = pickle.dumps( grades )\n",
    "\n",
    "#Use loads to de-serialize an object\n",
    "received_grades = pickle.loads( serial_grades )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "received_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "clf = SVC()\n",
    "X, y= datasets.load_iris(return_X_y=True)\n",
    "clf.fit(X, y)\n",
    "SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "s = pickle.dumps(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf2.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.predict(X[0:1])\n",
    "# np.array([0])\n",
    "# y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of joblib over pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf2, 'clf2.can')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = load('clf2.can') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Pipeline - demo1\n",
    "[source: 'https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "winedf = pd.read_csv('./winequality-red.csv')\n",
    "# print winedf.isnull().sum() # check for missing data\n",
    "winedf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=winedf.drop(['quality'],axis=1)\n",
    "Y=winedf['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('scaler', StandardScaler()), ('SVM', SVC())]\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=30, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(winedf['quality'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'SVM__C':[0.001,0.1,10,100,10e5], 'SVM__gamma':[0.1,0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipeline, param_grid=parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)\n",
    "print(\"score = %3.2f\" %(grid.score(X_test,y_test)))\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pipeline\n",
    "1. A pipeline defines all the steps such as transformation, modelling and hyperparameter tuning into one variable or object\n",
    "2. you can at once perform parameter tuning,scaling and modelling using the same object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Pipeline Activity/Demo -2 \n",
    "[source: 'https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pima indians dataset to:\n",
    "1. Create 3 PCA components\n",
    "\n",
    "2. Select 6 KBest features using k-best feature selection\n",
    "\n",
    "3. combine 6 best features selected with 3 PCA components to create new X\n",
    "\n",
    "4. Use Logistic Regression to classify diabetics with non diabetics\n",
    "\n",
    "5. Perform KFold CrossValidation\n",
    "\n",
    "6. Print cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(transformer_list=[('pca', PCA(n_components=3)),\n",
       "                               ('select_best', SelectKBest(k=6))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feature_union',\n",
       "  FeatureUnion(transformer_list=[('pca', PCA(n_components=3)),\n",
       "                                 ('select_best', SelectKBest(k=6))])),\n",
       " ('logistic', LogisticRegression())]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Arunabh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7773410799726589\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(estimators)\n",
    "# evaluate pipeline\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom class in sklearn pipeline\n",
    "\n",
    "[source: 'https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['X1', 'X2', 'y'], data=[\n",
    "                                                   [1,16,9],\n",
    "                                                   [4,36,16],\n",
    "                                                   [1,16,9],\n",
    "                                                   [2,9,8],\n",
    "                                                   [3,36,15],\n",
    "                                                   [2,49,16],\n",
    "                                                   [4,25,14],\n",
    "                                                   [5,36,17]\n",
    "])\n",
    "\n",
    "### y = X1 + 2 * sqrt(X2)\n",
    "\n",
    "train = df.iloc[:6]\n",
    "test = df.iloc[6:]\n",
    "\n",
    "train_X = train.drop('y', axis=1)\n",
    "train_y = train.y\n",
    "\n",
    "test_X = test.drop('y', axis=1)\n",
    "test_y = test.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[13.72113586 16.93334467]\n",
      "RMSE: 0.20274138822160784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m1 = LinearRegression()\n",
    "fit1 = m1.fit(train_X, train_y)\n",
    "preds = fit1.predict(test_X)\n",
    "print(f\"\\n{preds}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(test_y, preds))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predictions above are off\n",
    "* Let us try to transform X and fit a model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        print('\\n>>>>>>>transform() called.\\n')\n",
    "        X_ = X.copy() # creating a copy to avoid changes to original dataset\n",
    "        X_.X2 = 2 * np.sqrt(X_.X2)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create pipeline 1\n",
      "fit pipeline 1\n",
      "predict via pipeline 1\n",
      "\n",
      "[13.72113586 16.93334467]\n",
      "RMSE: 0.20274138822160784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without input transformation - to validate that we get the same results as before\n",
    "print(\"create pipeline 1\")\n",
    "pipe1 = Pipeline(steps=[\n",
    "                       ('linear_model', LinearRegression())\n",
    "])\n",
    "print(\"fit pipeline 1\")\n",
    "pipe1.fit(train_X, train_y)\n",
    "print(\"predict via pipeline 1\")\n",
    "preds1 = pipe1.predict(test_X)\n",
    "print(f\"\\n{preds1}\")  # should be [13.72113586 16.93334467]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(test_y, preds1))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create pipeline 2\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "fit pipeline 2\n",
      "\n",
      ">>>>>>>fit() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n",
      "predict via pipeline 2\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n",
      "\n",
      "[14. 17.]\n",
      "RMSE: 5.17892563931115e-15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with input transformation\n",
    "print(\"create pipeline 2\")\n",
    "pipe2 = Pipeline(steps=[\n",
    "                       ('experimental_trans', ExperimentalTransformer()),    # this will trigger a call to __init__\n",
    "                       ('linear_model', LinearRegression())\n",
    "])\n",
    "\n",
    "# an alternate, shorter syntax to do the above, without naming each step, is:\n",
    "#pipe2 = make_pipeline(ExperimentalTransformer(), LinearRegression())\n",
    "\n",
    "print(\"fit pipeline 2\")\n",
    "pipe2.fit(train_X, train_y)\n",
    "print(\"predict via pipeline 2\")\n",
    "preds2 = pipe2.predict(test_X)\n",
    "print(f\"\\n{preds2}\")  # should be [14. 17.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(test_y, preds2))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
