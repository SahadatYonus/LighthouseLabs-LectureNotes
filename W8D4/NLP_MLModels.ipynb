{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Machine Learning on Textual Data</center>\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise, we will preprocess the data with SpaCy based on the techniques we learnt in SpaCy introduction.\n",
    "\n",
    "Then, we will see a few application on the data.\n",
    "\n",
    "- TextClassification\n",
    "- Text Clustering\n",
    "- Sentiment Analysis\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The data set contains about 1000 online reviews each for various items on \n",
    "\n",
    "- Amazon, \n",
    "- Yelp and \n",
    "- IMDB, \n",
    "\n",
    "and of these reviews about 500 were labelled positive and 500 were labelled negative reviews. \n",
    "\n",
    "For each company, the data was given the text format which are needed to be added to a dataframe\n",
    "\n",
    "<a href=\"https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\">Dataset Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Text Classification</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data\n",
    "\n",
    "I. Yelp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0  1\n",
       "0                   Wow... Loved this place.  1\n",
       "1                         Crust is not good.  0\n",
       "2  Not tasty and the texture was just nasty.  0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yelp = pd.read_table('sentiment_data/yelp_labelled.txt', header = None)\n",
    "\n",
    "print(data_yelp.shape)\n",
    "\n",
    "data_yelp[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Amazon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  So there is no way for me to plug it in here i...  0\n",
       "1                        Good case, Excellent value.  1\n",
       "2                             Great for the jawbone.  1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_amazon = pd.read_table('sentiment_data/amazon_cells_labelled.txt', header = None)\n",
    "\n",
    "print(data_amazon.shape)\n",
    "\n",
    "data_amazon[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Imdb Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  A very, very, very slow-moving, aimless movie ...  0\n",
       "1  Not sure who was more lost - the flat characte...  0\n",
       "2  Attempting artiness with black & white and cle...  0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_imdb = pd.read_table('sentiment_data/imdb_labelled.txt', header = None)\n",
    "\n",
    "print(data_imdb.shape)\n",
    "\n",
    "data_imdb[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Amazon</th>\n",
       "      <th>500</th>\n",
       "      <td>The bose noise cancelling is amazing, which is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>This battery is an excellent bargain!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Defective crap.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">imdb</th>\n",
       "      <th>500</th>\n",
       "      <td>It's a case of 'so bad it is laughable'.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>) very bad performance plays Angela Bennett, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>It is a film about nothing, just a pretext to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">yelp</th>\n",
       "      <th>752</th>\n",
       "      <td>Level 5 spicy was perfect, where spice didn't ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>We were sat right on time and our server from ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>Main thing I didn't enjoy is that the crowd is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       review  label\n",
       "Amazon 500  The bose noise cancelling is amazing, which is...      1\n",
       "       501              This battery is an excellent bargain!      1\n",
       "       502                                    Defective crap.      0\n",
       "imdb   500         It's a case of 'so bad it is laughable'.        0\n",
       "       501  ) very bad performance plays Angela Bennett, a...      0\n",
       "       502  It is a film about nothing, just a pretext to ...      0\n",
       "yelp   752  Level 5 spicy was perfect, where spice didn't ...      1\n",
       "       753  We were sat right on time and our server from ...      1\n",
       "       754  Main thing I didn't enjoy is that the crowd is...      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack the 3 dataframes on top of another\n",
    "reviews = pd.concat([data_amazon, data_imdb, data_yelp], axis = 0, keys = [\"Amazon\", \"imdb\", \"yelp\"])\n",
    "\n",
    "# Set column names\n",
    "reviews.columns = ['review', 'label']\n",
    "\n",
    "reviews.iloc[[500, 501, 502, 1500, 1501, 1502, 2500, 2501, 2502]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "We will implement the following steps in our preprocessing pipeline.\n",
    "\n",
    "- Tokenisation\n",
    "- Lemmatization\n",
    "- Stop Words Removal\n",
    "- Punctuations Removal\n",
    "- Vectorisation\n",
    "\n",
    "#### Create Stop Words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from  spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# To build a list of stop words for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thereby', 'beforehand', 'amount', 'well', 'twenty', 'such', 'regarding', 'out', 'unless', 'again', 'fifty', 'than', 'something', 'therefore', '‘ve', 'others', 'only', 'in', 'three', \"'re\", 'but', 'cannot', 're', 'become', 'where', '’m', '‘ll', 'ever', 'everyone', 'never', 'becoming', 'make', '’s', 'now', 'has', 'not', 'their', 'next', 'no', \"'m\", 'whereas', 'show', 'very', 'might', 'per', 'until', 'whenever', 'although', 'as', 'name', 'everywhere', 'there', '‘re', 'throughout', 'some', 'hers', 'with', 'always', 'perhaps', 'elsewhere', 'somehow', 'empty', 'sometime', 'all', 'due', 'do', 'either', 'further', 'third', 'none', 'seemed', 'whole', 'must', 'me', 'call', 'i', 'indeed', 'thus', 'here', 'had', 'whatever', 'from', 'five', 'when', 'nobody', 'alone', 'latterly', 'why', '’d', 'side', 'whether', 'nothing', 'done', 'my', 'several', 'each', 'serious', 'nevertheless', 'wherever', 'whereby']\n"
     ]
    }
   ],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "print(stopwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if your word in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('which' in stopwords)\n",
    "print('thing' in stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the word `thing` to the stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thing', 'thereby', 'beforehand', 'amount', 'well']\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['thing'] + list(STOP_WORDS)\n",
    "\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Punctuation list\n",
    "\n",
    "We will use the default punctuation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create preprocessing Pipeline\n",
    "\n",
    "Tokenized words needs to be lemmatized and filtered for pronouns, stopwords and punctuations using the defined method 'preprocess'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to tokenise the text\n",
    "# Define a function to handle all data cleaning\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    #lememtization\n",
    "    #stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the how the preprocess function will work for a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saying', 'says', 'lot', 'things', 'wasnt', 'paying', 'attention']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"He was saying some thing but he says a lot of things so I just wasn't paying attention.\" # :D \n",
    "\n",
    "clean_text(sent) \n",
    "\n",
    "# After pre-processing we have only one token left in the list. Saves so much computation. Pheww.\n",
    "# Remove the rule 3 and see the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a vectorizer object and pass the preprocess function we created to the tokeniser argument\n",
    "tfvectorizer = TfidfVectorizer(tokenizer = clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Support Vector Classifier Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Initialise Model Object\n",
    "classifier = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - Test Split\n",
    "\n",
    "The data is split into training and test datasets prior to feeding into the machine learning pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( reviews['review'], reviews['label'], \n",
    "                                                    test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape :  (2198,)\n",
      "Test Shape :  (550,)\n"
     ]
    }
   ],
   "source": [
    "print('Train Shape : ', X_train.shape)\n",
    "\n",
    "print('Test Shape : ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the  pipeline to clean, tokenize, vectorize, and classify using\"Count Vectorizor\"\n",
    "# Multiple models can be added to the Pipeline object to be executed in sequence.\n",
    "model_pipe = Pipeline( [ ('vectorizer', tfvectorizer), \n",
    "                         ('classifier', classifier) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function clean_text at 0x00000191D72BB948>)),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model_pipe.predict(X_test)\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's start with all the problems\\x97the acting, especially from the lead professor, was very, very bad.  \""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9499545040946314\n",
      "Test Accuracy:  0.7963636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# Accuracy\n",
    "print(\"Train Accuracy: \", model_pipe.score(X_train, y_train))\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy: \", model_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <center>Text Clustering</center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfvectorizer.fit_transform(reviews['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '0',\n",
       " '010',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1010',\n",
       " '11',\n",
       " '110',\n",
       " '1199',\n",
       " '12',\n",
       " '13',\n",
       " '15',\n",
       " '15lb',\n",
       " '17',\n",
       " '18',\n",
       " '18th',\n",
       " '1928',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1971',\n",
       " '1973',\n",
       " '1979',\n",
       " '1980s',\n",
       " '1986',\n",
       " '1995',\n",
       " '1998',\n",
       " '2',\n",
       " '20',\n",
       " '2000',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '20th',\n",
       " '20the',\n",
       " '2160',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '2mp',\n",
       " '3',\n",
       " '30',\n",
       " '30s',\n",
       " '325',\n",
       " '34ths',\n",
       " '35',\n",
       " '350',\n",
       " '375',\n",
       " '3o',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '40min',\n",
       " '42',\n",
       " '45',\n",
       " '4s',\n",
       " '5',\n",
       " '50',\n",
       " '5020',\n",
       " '510',\n",
       " '5320',\n",
       " '54',\n",
       " '5of',\n",
       " '5year',\n",
       " '6',\n",
       " '680',\n",
       " '7',\n",
       " '70',\n",
       " '70000',\n",
       " '700w',\n",
       " '70s',\n",
       " '744',\n",
       " '750',\n",
       " '785',\n",
       " '8',\n",
       " '80',\n",
       " '80s',\n",
       " '810',\n",
       " '8125',\n",
       " '815pm',\n",
       " '8525',\n",
       " '8530',\n",
       " '8pm',\n",
       " '9',\n",
       " '90',\n",
       " '90s',\n",
       " '910',\n",
       " '95',\n",
       " 'aailiyah',\n",
       " 'abandoned',\n",
       " 'abhor',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abound',\n",
       " 'abovepretty',\n",
       " 'abroad',\n",
       " 'absolute',\n",
       " 'absolutel',\n",
       " 'absolutely',\n",
       " 'absolutley',\n",
       " 'abstruse',\n",
       " 'abysmal',\n",
       " 'ac',\n",
       " 'academy',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'access',\n",
       " 'accessable',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accessory',\n",
       " 'accessoryone',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'acclaimed',\n",
       " 'accolades',\n",
       " 'accommodations',\n",
       " 'accomodate',\n",
       " 'accompanied',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accountant',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accused',\n",
       " 'ache',\n",
       " 'achievement',\n",
       " 'achille',\n",
       " 'ackerman',\n",
       " 'acknowledged',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'actingeven',\n",
       " 'actingwise',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activesync',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actorsan',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adams',\n",
       " 'adaptation',\n",
       " 'adapter',\n",
       " 'adapters',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adhesive',\n",
       " 'admins',\n",
       " 'admiration',\n",
       " 'admitted',\n",
       " 'adorable',\n",
       " 'adorablehis',\n",
       " 'adorablethe',\n",
       " 'adrift',\n",
       " 'adventure',\n",
       " 'advertised',\n",
       " 'advise',\n",
       " 'aerial',\n",
       " 'aesthetically',\n",
       " 'affected',\n",
       " 'affleck',\n",
       " 'affordable',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'ages',\n",
       " 'aggravating',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ahead',\n",
       " 'aimless',\n",
       " 'air',\n",
       " 'aired',\n",
       " 'airline',\n",
       " 'airport',\n",
       " 'akasha',\n",
       " 'akin',\n",
       " 'ala',\n",
       " 'alarm',\n",
       " 'albondigas',\n",
       " 'alert',\n",
       " 'alexander',\n",
       " 'alike',\n",
       " 'allergy',\n",
       " 'allison',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allstar',\n",
       " 'almonds',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'aluminum',\n",
       " 'amateurish',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazingrge',\n",
       " 'amazingstylized',\n",
       " 'amazon',\n",
       " 'ambiance',\n",
       " 'ambience',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'amp',\n",
       " 'ample',\n",
       " 'amusing',\n",
       " 'anatomist',\n",
       " 'andddd',\n",
       " 'andor',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelina',\n",
       " 'angle',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'angus',\n",
       " 'animals',\n",
       " 'animated',\n",
       " 'animation',\n",
       " 'anita',\n",
       " 'ann',\n",
       " 'anne',\n",
       " 'annes',\n",
       " 'anniversary',\n",
       " 'annoying',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'antena',\n",
       " 'anthony',\n",
       " 'anticipated',\n",
       " 'antiglare',\n",
       " 'antithesis',\n",
       " 'anymore',\n",
       " 'anythinga',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'appalling',\n",
       " 'apparently',\n",
       " 'appealing',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'appetite',\n",
       " 'appetizer',\n",
       " 'appetizers',\n",
       " 'applauded',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'applifies',\n",
       " 'appointments',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'apt',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'arepas',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'aria',\n",
       " 'armageddon',\n",
       " 'armand',\n",
       " 'armband',\n",
       " 'array',\n",
       " 'arrival',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'art',\n",
       " 'article',\n",
       " 'articulated',\n",
       " 'artiness',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artless',\n",
       " 'arts',\n",
       " 'asia',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'ass',\n",
       " 'assante',\n",
       " 'assaulted',\n",
       " 'assistant',\n",
       " 'assumed',\n",
       " 'assure',\n",
       " 'astonishingly',\n",
       " 'astronaut',\n",
       " 'astronauts',\n",
       " 'ate',\n",
       " 'atleast',\n",
       " 'atmosphere',\n",
       " 'atmosphere1',\n",
       " 'atrocious',\n",
       " 'atrocity',\n",
       " 'att',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attempts',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'attitudes',\n",
       " 'attractive',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'auju',\n",
       " 'aurvåg',\n",
       " 'austens',\n",
       " 'austere',\n",
       " 'authentic',\n",
       " 'author',\n",
       " 'auto',\n",
       " 'autoanswer',\n",
       " 'available',\n",
       " 'average',\n",
       " 'aversion',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'awsome',\n",
       " 'ayce',\n",
       " 'aye',\n",
       " 'az',\n",
       " 'b',\n",
       " 'baaaaaad',\n",
       " 'baba',\n",
       " 'babbling',\n",
       " 'babie',\n",
       " 'baby',\n",
       " 'babysitting',\n",
       " 'bachi',\n",
       " 'backdrop',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backlight',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badly',\n",
       " 'badwellits',\n",
       " 'bagels',\n",
       " 'baileys',\n",
       " 'bakery',\n",
       " 'baklava',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'ball',\n",
       " 'ballet',\n",
       " 'balls',\n",
       " 'bamboo',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bank',\n",
       " 'bar',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barking',\n",
       " 'barney',\n",
       " 'barren',\n",
       " 'bars',\n",
       " 'bartender',\n",
       " 'bartenders',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bates',\n",
       " 'bathroom',\n",
       " 'bathrooms',\n",
       " 'batter',\n",
       " 'batteries',\n",
       " 'battery',\n",
       " 'baxendale',\n",
       " 'bay',\n",
       " 'bbq',\n",
       " 'be3',\n",
       " 'beall',\n",
       " 'bean',\n",
       " 'beans',\n",
       " 'bear',\n",
       " 'beat',\n",
       " 'beateous',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'bechard',\n",
       " 'bed',\n",
       " 'beef',\n",
       " 'beensteppedinandtrackedeverywhere',\n",
       " 'beep',\n",
       " 'beeping',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'beforei',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'behing',\n",
       " 'behold',\n",
       " 'bela',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'bell',\n",
       " 'bellagio',\n",
       " 'bellies',\n",
       " 'bells',\n",
       " 'bellucci',\n",
       " 'belly',\n",
       " 'belmondo',\n",
       " 'belowpar',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'bend',\n",
       " 'bennett',\n",
       " 'bergens',\n",
       " 'bertolucci',\n",
       " 'best',\n",
       " 'bethe',\n",
       " 'better',\n",
       " 'betty',\n",
       " 'beware',\n",
       " 'bible',\n",
       " 'big',\n",
       " 'bigbudget',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bill',\n",
       " 'bills',\n",
       " 'billy',\n",
       " 'binge',\n",
       " 'biographical',\n",
       " 'bipolarity',\n",
       " 'bird',\n",
       " 'biscuit',\n",
       " 'biscuits',\n",
       " 'bisque',\n",
       " 'bit',\n",
       " 'bitches',\n",
       " 'bitchy',\n",
       " 'bite',\n",
       " 'bites',\n",
       " 'bitpim',\n",
       " 'bits',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blacks',\n",
       " 'blacktop',\n",
       " 'blah',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'bland',\n",
       " 'blandest',\n",
       " 'blandly',\n",
       " 'blanket',\n",
       " 'blare',\n",
       " 'blatant',\n",
       " 'blew',\n",
       " 'blist',\n",
       " 'block',\n",
       " 'bloddy',\n",
       " 'blood',\n",
       " 'bloodiest',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blown',\n",
       " 'blows',\n",
       " 'blue',\n",
       " 'blueant',\n",
       " 'bluegreenscreen',\n",
       " 'bluetoooth',\n",
       " 'bluetooth',\n",
       " 'bluetoothmotorola',\n",
       " 'bluetooths',\n",
       " 'blush',\n",
       " 'bmw',\n",
       " 'boasts',\n",
       " 'bob',\n",
       " 'boba',\n",
       " 'bodes',\n",
       " 'body',\n",
       " 'bohemian',\n",
       " 'boiled',\n",
       " 'boiling',\n",
       " 'bold',\n",
       " 'bombardments',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'bonuses',\n",
       " 'boobs',\n",
       " 'boogeyman',\n",
       " 'book',\n",
       " 'booking',\n",
       " 'booksomethats',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'bop',\n",
       " 'bordered',\n",
       " 'borderlines',\n",
       " 'borders',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'boringpointless',\n",
       " 'borrowed',\n",
       " 'bose',\n",
       " 'boss',\n",
       " 'bother',\n",
       " 'bothersome',\n",
       " 'bottowm',\n",
       " 'bouchon',\n",
       " 'bought',\n",
       " 'bougth',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boxes',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boyle',\n",
       " 'boys',\n",
       " 'brain',\n",
       " 'brainsucking',\n",
       " 'brand',\n",
       " 'brat',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakage',\n",
       " 'breakfast',\n",
       " 'breakfastlunch',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breeders',\n",
       " 'breeze',\n",
       " 'brevity',\n",
       " 'brian',\n",
       " 'brick',\n",
       " 'brief',\n",
       " 'brigand',\n",
       " 'bright',\n",
       " 'brilliance',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'bring',\n",
       " 'brings',\n",
       " 'brisket',\n",
       " 'broad',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brokeni',\n",
       " 'brooding',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brownish',\n",
       " 'browser',\n",
       " 'browsing',\n",
       " 'brunch',\n",
       " 'bruschetta',\n",
       " 'brushfire',\n",
       " 'brutal',\n",
       " 'bt',\n",
       " 'bt250v',\n",
       " 'bt50',\n",
       " 'bubbling',\n",
       " 'bucks',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buds',\n",
       " 'buffalo',\n",
       " 'buffet',\n",
       " 'buffets',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'builders',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'buildup',\n",
       " 'built',\n",
       " 'buldogis',\n",
       " 'bulky',\n",
       " 'bullock',\n",
       " 'bully',\n",
       " 'bumpers',\n",
       " 'bunch',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'burned',\n",
       " 'burrittos',\n",
       " 'burtons',\n",
       " 'bus',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'bussell',\n",
       " 'busy',\n",
       " 'butter',\n",
       " 'button',\n",
       " 'buttons',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buyerbe',\n",
       " 'buyers',\n",
       " 'buying',\n",
       " 'buyit',\n",
       " 'buzzing',\n",
       " 'bye',\n",
       " 'ca42',\n",
       " 'caballeros',\n",
       " 'cable',\n",
       " 'cables',\n",
       " 'caesar',\n",
       " 'cafe',\n",
       " 'café',\n",
       " 'cailles',\n",
       " 'cakeohhh',\n",
       " 'cakes',\n",
       " 'calamari',\n",
       " 'calendar',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calligraphy',\n",
       " 'callings',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'camelback',\n",
       " 'cameo',\n",
       " 'camera',\n",
       " 'camerawork',\n",
       " 'camp',\n",
       " 'campy',\n",
       " 'canada',\n",
       " 'canal',\n",
       " 'cancan',\n",
       " 'cancellation',\n",
       " 'cancelling',\n",
       " 'candace',\n",
       " 'candle',\n",
       " 'cando',\n",
       " 'cannoli',\n",
       " 'cant',\n",
       " 'capability',\n",
       " 'capacity',\n",
       " 'cape',\n",
       " 'capers',\n",
       " 'captain',\n",
       " 'captured',\n",
       " 'captures',\n",
       " 'car',\n",
       " 'carbs',\n",
       " 'card',\n",
       " 'cardboard',\n",
       " 'cardellini',\n",
       " 'care',\n",
       " 'careful',\n",
       " 'caring',\n",
       " 'carlys',\n",
       " 'carol',\n",
       " 'carpaccio',\n",
       " 'carrell',\n",
       " 'carried',\n",
       " 'carriers',\n",
       " 'carries',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'cart',\n",
       " 'cartel',\n",
       " 'cartoon',\n",
       " 'cartoons',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cash',\n",
       " 'cashew',\n",
       " 'cashier',\n",
       " 'casing',\n",
       " 'casino',\n",
       " 'cassette',\n",
       " 'cast',\n",
       " 'casted',\n",
       " 'casting',\n",
       " 'cat',\n",
       " 'catching',\n",
       " 'catchy',\n",
       " 'caterpillar',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'cavier',\n",
       " 'cbr',\n",
       " 'cds',\n",
       " 'ceases',\n",
       " 'celebration',\n",
       " 'celebrity',\n",
       " 'cell',\n",
       " 'cellphone',\n",
       " 'cellphones',\n",
       " 'cellular',\n",
       " 'celluloid',\n",
       " 'center',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'cents',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'cg',\n",
       " 'cgi',\n",
       " 'chai',\n",
       " 'chains',\n",
       " 'chalkboard',\n",
       " 'challenges',\n",
       " 'chance',\n",
       " 'chanceit',\n",
       " 'change',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'char',\n",
       " 'character',\n",
       " 'characterage',\n",
       " 'characterisation',\n",
       " 'characters',\n",
       " 'charcoal',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'chargelife',\n",
       " 'charger',\n",
       " 'chargers',\n",
       " 'charges',\n",
       " 'charging',\n",
       " 'charisma',\n",
       " 'charismafree',\n",
       " 'charismatic',\n",
       " 'charles',\n",
       " 'charlie',\n",
       " 'charm',\n",
       " 'charming',\n",
       " 'chase',\n",
       " 'chasing',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'cheaply',\n",
       " 'cheapy',\n",
       " 'cheated',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheek',\n",
       " 'cheekbones',\n",
       " 'cheerfull',\n",
       " 'cheerless',\n",
       " 'cheese',\n",
       " 'cheeseburger',\n",
       " 'cheesecurds',\n",
       " 'cheesiness',\n",
       " 'cheesy',\n",
       " 'chef',\n",
       " 'chefs',\n",
       " 'chemistry',\n",
       " 'chewy',\n",
       " 'chick',\n",
       " 'chicken',\n",
       " 'chickens',\n",
       " 'chickenwith',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'childlike',\n",
       " 'children',\n",
       " 'childrens',\n",
       " 'chills',\n",
       " 'chilly',\n",
       " 'chimplike',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chip',\n",
       " 'chipolte',\n",
       " 'chipotle',\n",
       " 'chips',\n",
       " 'chocolate',\n",
       " 'chodorov',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choked',\n",
       " 'choose',\n",
       " 'chosen',\n",
       " 'choux',\n",
       " 'chow',\n",
       " 'christmas',\n",
       " 'christopher',\n",
       " 'church',\n",
       " 'cibo',\n",
       " 'cinema',\n",
       " 'cinematic',\n",
       " 'cinematographers',\n",
       " 'cinematography',\n",
       " 'cinematographyif',\n",
       " 'cingulair',\n",
       " 'cingular',\n",
       " 'cingularatt',\n",
       " 'circumstances',\n",
       " 'claimed',\n",
       " 'clarity',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'classical',\n",
       " 'classics',\n",
       " 'classy',\n",
       " 'classywarm',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearer',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'clichés',\n",
       " 'clicks',\n",
       " 'clients',\n",
       " 'cliff',\n",
       " 'climax',\n",
       " 'climbing',\n",
       " 'clip',\n",
       " 'clipping',\n",
       " 'clips',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closeup',\n",
       " 'closeups',\n",
       " 'clothes',\n",
       " 'club',\n",
       " 'clue',\n",
       " 'coach',\n",
       " 'coal',\n",
       " 'coastal',\n",
       " 'coaster',\n",
       " 'cocktail',\n",
       " 'cocktails',\n",
       " 'coconut',\n",
       " 'cod',\n",
       " 'coffee',\n",
       " 'coherent',\n",
       " 'cold',\n",
       " 'colder',\n",
       " 'cole',\n",
       " 'colleague',\n",
       " 'collect',\n",
       " 'collective',\n",
       " 'college',\n",
       " 'color',\n",
       " 'colored',\n",
       " 'colorful',\n",
       " 'colors',\n",
       " 'colours',\n",
       " 'columbo',\n",
       " 'combination',\n",
       " 'combo',\n",
       " 'combos',\n",
       " 'come',\n",
       " 'comedic',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comfortably',\n",
       " 'comfortible',\n",
       " 'comforting',\n",
       " 'comical',\n",
       " 'coming',\n",
       " 'commands',\n",
       " 'comment',\n",
       " 'commentary',\n",
       " 'commented',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'commercials',\n",
       " 'common',\n",
       " 'communicate',\n",
       " 'communication',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'commuter',\n",
       " 'companions',\n",
       " 'company',\n",
       " 'companys',\n",
       " 'comparablypriced',\n",
       " 'compared',\n",
       " 'compelling',\n",
       " 'compete',\n",
       " 'competent',\n",
       " 'competitors',\n",
       " 'complain',\n",
       " 'complained',\n",
       " 'complaint',\n",
       " 'complaints',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complexity',\n",
       " 'compliments',\n",
       " 'composed',\n",
       " 'composition',\n",
       " 'comprehensible',\n",
       " 'compromise',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'concentrate',\n",
       " 'concept',\n",
       " 'conception',\n",
       " 'conceptually',\n",
       " 'concern',\n",
       " 'concerning',\n",
       " 'concerns',\n",
       " 'concert',\n",
       " 'conclusion',\n",
       " 'concrete',\n",
       " 'condescends',\n",
       " 'condiment',\n",
       " 'conditions',\n",
       " 'confidence',\n",
       " 'configuration',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'confortable',\n",
       " 'confuses',\n",
       " 'confusing',\n",
       " 'connect',\n",
       " 'connected',\n",
       " 'connecting',\n",
       " 'connection',\n",
       " 'connections',\n",
       " 'connery',\n",
       " 'connerys',\n",
       " 'connoisseur',\n",
       " 'conrad',\n",
       " 'consequences',\n",
       " 'consider',\n",
       " 'considerable',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'considers',\n",
       " 'consistent',\n",
       " 'consolations',\n",
       " 'constant',\n",
       " 'constantine',\n",
       " 'constantly',\n",
       " 'constructed',\n",
       " 'construction',\n",
       " 'consumer',\n",
       " 'contact',\n",
       " 'contacted',\n",
       " 'contacting',\n",
       " 'contacts',\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert tfidf matrix to dense form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2748, 5146)\n",
      "<class 'numpy.matrix'>\n"
     ]
    }
   ],
   "source": [
    "dense = tfidf.todense()\n",
    "print(dense.shape)\n",
    "print(type(dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the kmeans model and retrieve labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Creating cluster using the kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++')\n",
    "\n",
    "model = kmeans.fit(dense)\n",
    "labels = model.labels_\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the frequency count for each value in the label\n",
    "\n",
    "We can see the model has not done a good job as it has predicted most values in one of the cluster. As this is review data for different companies, the context might be quite similar. It might just be clustering similar length sentences together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2426\n",
       "1     171\n",
       "2     151\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(labels)[0].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
